{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4358d166-dc16-444a-8a41-3224fa3c0b49",
   "metadata": {},
   "source": [
    "# Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88713c8d-b3d5-4804-8809-fa6cdfae13f2",
   "metadata": {},
   "source": [
    "## Acknowledgement\n",
    "\n",
    "This notebook is based off: https://github.com/gkamradt/langchain-tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dad0c20-46ac-4026-8bb9-a1b51d243789",
   "metadata": {},
   "source": [
    "#### SageMaker Studio Notebook\n",
    "This notebook has been tested in us-east-1 with **Data Science 3.0** kernel\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66bd920-8269-4848-8f47-73e2b701b63b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install transformers faiss-gpu --quiet\n",
    "!pip install bs4 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13d9d32-d207-4b30-8224-6b54f9daf19c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install upgrade sagemaker\n",
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1474e4e8-896e-4fa1-ad82-561c7c9ad439",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import langchain\n",
    "import requests\n",
    "import json\n",
    "\n",
    "print(langchain.__version__)\n",
    "# assert int(langchain.__version__.split(\".\")[-1]) >= 194"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be66f15-e11e-4aeb-abb1-61ae9af84f08",
   "metadata": {},
   "source": [
    "# Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25faf96-1871-440e-8ea8-724ecf39cea7",
   "metadata": {},
   "source": [
    "### Deploy from SageMaker JumpStart- We will deploy textembedding-gpt-6b-fp16 in this lab.\n",
    "\n",
    "#### <mark> Step 1</mark>, we go to SageMaker Jumpstart and deploy one of the gpt-j embedding models- we will use this model further down to generate embeddings.\n",
    "\n",
    "- <mark> textembedding-gpt-j-6b-fp16 (ml.g5.4xlarge)</mark> #this notebook deploys and uses this one.\n",
    "or\n",
    "- textembedding-gpt-j-6b (ml.g5.12xlarge)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71387ff-39ef-47eb-b361-b946dc2e43bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_MODEL_CONFIG_ = {\n",
    "    \n",
    "    \"textembedding-model\": {\n",
    "        \"aws_region\": \"us-east-1\",\n",
    "        \"endpoint_name\": \"jumpstart-dft-textembedding-gpt-j-6b-fp16-1\",\n",
    "    },\n",
    "    \n",
    "    \"llm-model\" : {\n",
    "        \"aws_region\": \"us-east-1\",\n",
    "        \"endpoint_name\": \"demo-Falcon40B-Endpoint\",\n",
    "        \"api_url\": \"https://t7zr78elmj.execute-api.us-east-1.amazonaws.com/prod/falcon\",\n",
    "        \"headers\":{\n",
    "    'Content-Type': 'application/json',\n",
    "    'Accept': 'application/json',\n",
    "    'Authorization': 'xxx'  #insert the authentication code\n",
    "}\n",
    "\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b439640-d75e-4836-9417-84b72c8ebf2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Falcon 40B model has been already delpoyed on a ml.g5.24x instance and the url where you can invoke and interact with the model is provided in the following together with and the authentication passed on as part of the message. see the header that contains authentication inforamtion in our llm model configuration we defined above ( a copy in the following)    \n",
    "<code>\n",
    "        \"llm-model\" : {\n",
    "        \"aws_region\": \"us-east-1\",\n",
    "        \"endpoint_name\": \"demo-Falcon40B-Endpoint\",\n",
    "        \"api_url\": \"https://t7zr78elmj.execute-api.us-east-1.amazonaws.com/prod/falcon\",\n",
    "        \"headers\":{\n",
    "        'Content-Type': 'application/json',\n",
    "        'Accept': 'application/json',\n",
    "        'Authorization': 'xxxx'\n",
    "    }\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e47c49-bfe0-4e9f-9863-c1fe8c37381f",
   "metadata": {},
   "source": [
    "# Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b34dfd-dea7-42d4-92aa-55dc443a1e80",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Schema\n",
    "This section covers the basic data types and schemas used in Langchain.\n",
    "There are few data type/ schema supported by langchain- we will explore text and documents in the following section.\n",
    "- text\n",
    "- document\n",
    "- examples\n",
    "- chat messages (covered in optional section)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e3d53e-2619-4f22-94cd-1127da1075c0",
   "metadata": {},
   "source": [
    "## Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b4b410-7923-4c2f-bc7d-fef69dfc0306",
   "metadata": {
    "tags": []
   },
   "source": [
    "You'll be working with simple strings - spoiler alert: that'll soon grow in complexity! ;) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e058ae-046d-4746-af11-5005816cfcb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_text = \"What day comes after Friday?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00208a91-4220-4945-b433-b4cfc3cece66",
   "metadata": {},
   "source": [
    "## Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e70acac-8d8a-415a-be73-781cd8491603",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\",\n",
    "         metadata={\n",
    "             'my_document_id' : 234234,\n",
    "             'my_document_source' : \"The LangChain Papers\",\n",
    "             'my_document_create_time' : 1680013019\n",
    "         })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7560f5d-d458-4227-bba6-b05a7e3ec790",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Models\n",
    "There are three type of models in Lanchain- Language Model, Chat model and text Embedding Model- in the following we will interact with text embedding and language\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c654180-c902-4553-b0e2-4b1ae9c27b8d",
   "metadata": {},
   "source": [
    "## Language Models\n",
    "Lets invoke our Flacon model using a simple string prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36088b88-472e-4e11-af40-50c7a10a9067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import AmazonAPIGateway\n",
    "\n",
    "parameters ={\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"top_k\": 50,\n",
    "        \"top_p\": 0.95,\n",
    "        \"do_sample\": False,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.2\n",
    "}\n",
    "\n",
    "\n",
    "sm_llm_falcon_instruct = AmazonAPIGateway(\n",
    "    api_url=_MODEL_CONFIG_[\"llm-model\"][\"api_url\"], \n",
    "    model_kwargs=parameters,\n",
    "    headers=_MODEL_CONFIG_[\"llm-model\"][\"headers\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d898ee1b-fce5-4af5-93a9-94fcd7b4f509",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_llm_falcon_instruct(\"what is Amazon SageMaker?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d96b310-f31f-43f2-88e4-77e59195cfd2",
   "metadata": {},
   "source": [
    "## Text Embedding Models\n",
    "\n",
    "We need to Wrap up our SageMaker endpoints for embedding model into <mark>langchain.embeddings.SagemakerEndpointEmbeddings</mark>. That requires a small overwritten of SagemakerEndpointEmbeddings class to make it compatible with SageMaker embedding mdoel. Same applies to use the SageMaker endpoint for Language models- In the above you notice we are using the APIGateway class for the Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce36a22-b4ba-49d5-86a0-84db445cd901",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "import json\n",
    "\n",
    "\n",
    "class ContentHandler(EmbeddingsContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, inputs: List[str], model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({\"text_inputs\": inputs, **model_kwargs})\n",
    "        return input_str.encode('utf-8')\n",
    "\n",
    "    def transform_output(self, output: bytes) -> List[List[float]]:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"embedding\"]\n",
    "\n",
    "emb_content_handler = ContentHandler()\n",
    "\n",
    "\n",
    "embeddings = SagemakerEndpointEmbeddings(\n",
    "    endpoint_name=_MODEL_CONFIG_[\"textembedding-model\"][\"endpoint_name\"],\n",
    "    region_name=_MODEL_CONFIG_[\"textembedding-model\"][\"aws_region\"],\n",
    "    content_handler=emb_content_handler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d12adf-eb2f-48ce-b77e-1440ec2e4bfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"Hi! It's time for the beach\"\n",
    "\n",
    "text_embedding = embeddings.embed_query(text)\n",
    "print (f\"Your embedding is length {len(text_embedding)}\")\n",
    "print (f\"Here's a sample: {text_embedding[:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4215e75-9ea0-466a-89b8-95a0d7fe8974",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc_embedding = embeddings.embed_documents([text])\n",
    "print (f\"Your embedding is length {len(doc_embedding[0])}\")\n",
    "print (f\"Here's a sample: {doc_embedding[0][:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82c2308-7cd1-4f4e-840a-cf179c18b5e0",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "The new way of programming models is through prompts. A \"prompt\" refers to the input to the model. This input is rarely hard coded, but rather is often constructed from multiple components.\n",
    "This is an example of a simple text prompt, passed to our model to get some response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb3b029-651c-4111-b845-0cb49794cb94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Today is Monday, tomorrow is Wednesday.\n",
    "\n",
    "What is wrong with that statement?\n",
    "\"\"\"\n",
    "\n",
    "sm_llm_falcon_instruct(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19740785-c886-4d0c-a738-a852c505c805",
   "metadata": {},
   "source": [
    "## Prompt Template\n",
    "This is an example of a simple prompt template, passed on to our model to get some response.\n",
    "A PromptValue is what is eventually passed to the model. Most of the time, this value is not hardcoded but is rather dynamically created based on a combination of user input, other non-static information (often coming from multiple sources), and a fixed template string. We call the object responsible for creating the PromptValue a PromptTemplate. This object exposes a method for taking in input variables and returning a PromptValue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a75ed87-ac02-469b-95f8-377a3088ff78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# Notice \"location\" below, that is a placeholder for another value later\n",
    "template = \"\"\"\n",
    "I really want to travel to {location}. What should I do there?\n",
    "\n",
    "Respond in one short sentence\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"location\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(location='Melbourne')\n",
    "\n",
    "print (f\"Final Prompt: {final_prompt}\")\n",
    "print (\"-----------\")\n",
    "print (f\"LLM Output: {sm_llm_falcon_instruct(final_prompt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf1f7dd-246e-4f61-b080-7e16c49565f3",
   "metadata": {},
   "source": [
    "# Chain\n",
    "Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case. The most commonly used type of chain is an LLMChain, which combines a PromptTemplate, a Model, and Guardrails to take user input, format it accordingly, pass it to the model and get a response, and then validate and fix (if necessary) the model output.\n",
    "\n",
    "The following shows few examples of chains that combines a promot tempalte and a LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9185f928-d359-42a1-81c8-025c0cc7cf09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5881fd85-292d-4bd2-910b-a068c291cc69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"max_new_tokens\": 1500,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": False,\n",
    "    \"temperature\": 1,\n",
    "    \"seed\": 123\n",
    "}\n",
    "\n",
    "sm_llm_falcon_instruct.model_kwargs = parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99724637-6fea-4db9-96d4-7f5ad9bd4824",
   "metadata": {},
   "source": [
    "### LLMChain: \n",
    "LLM Chain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format. This is a simple LLMchain that consists of a prompt template and a language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3079d7c-852a-44f6-b272-54b736210a53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Your job is to come up with a classic dish from the area that the users suggests.\n",
    "% USER LOCATION\n",
    "{user_location}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_location\"], template=template)\n",
    "\n",
    "# Holds my 'location' chain\n",
    "location_chain = LLMChain(llm=sm_llm_falcon_instruct, prompt=prompt_template)\n",
    "output=location_chain.run(\"Melbourne\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834731f6-cf9f-44c0-8cb7-b21b39a66980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Given a meal, give a short and simple recipe on how to make that dish at home.\n",
    "% MEAL\n",
    "{user_meal}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_meal\"], template=template)\n",
    "\n",
    "# Holds my 'meal' chain\n",
    "meal_chain = LLMChain(llm=sm_llm_falcon_instruct, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518b17c4-0d45-4429-92a9-05ba6443182b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Now an example of chain, that sequentally chain two LLMs together with the user prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efc9003-c646-428f-8372-45528704c5ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "overall_chain = SimpleSequentialChain(chains=[location_chain, meal_chain], verbose=True)\n",
    "review = overall_chain.run(\"Melbourne\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ca9b01-74a0-48e2-b3cd-c7b44f0ee757",
   "metadata": {},
   "source": [
    "### Summarization Chain\n",
    "An example of Summarization chain is created in the following.\n",
    "- first we use documents as our input ( other input format beyond the simple text we discussed above)- we use document_loader class to load the document\n",
    "- second, we split the text inot chunks before feeding it into the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53b2812-938c-4e9b-a502-69ad9d075bae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = TextLoader('data/Amazon_SageMaker_FAQs.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=5)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73fde4d-faab-46a6-93be-5d7172cd2646",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"max_new_tokens\": 100,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": False,\n",
    "    \"temperature\": 1,\n",
    "}\n",
    "\n",
    "sm_llm_falcon_instruct.model_kwargs = parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597a3723-72e1-484f-a02e-c7905e5a1386",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# There is a lot of complexity hidden in this one line. I encourage you to check out the video above for more detail\n",
    "# chain = load_summarize_chain(sm_llm, chain_type=\"map_reduce\", verbose=True)\n",
    "chain = load_summarize_chain(sm_llm_falcon_instruct, chain_type=\"map_reduce\", verbose=True)\n",
    "chain.run(texts[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4248eab-a142-4e77-9cd0-71ab10328225",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### There is a lot more to learn and discover with Langchains, inclusing output parsers, agents, tools, growing rapidly. You can learn more about Langchain capabilities here https://github.com/gkamradt/langchain-tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0222be4-d7d6-44ef-acdb-522b5a10e406",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "- Delete deployed LLM endpoint"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
