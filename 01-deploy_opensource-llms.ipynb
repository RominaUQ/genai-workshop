{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01ca8a72-9207-442f-82b1-a8a411267694",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deploy open-source Large Language Models on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47da96f9-24d5-4c12-b867-83be52d71d76",
   "metadata": {},
   "source": [
    "#### Before we start\n",
    "This notebook runs with <mark>Data Science 3.0 </mark> Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f8a66a-d764-452b-94e1-2fd614e72970",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tiktoken\n",
    "import json\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7432194-fede-4600-aab4-2b537226b390",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Section 1: Deploy Llama model from SageMaker jumpstart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef342e8-24a3-4b86-b354-5d3f6123e255",
   "metadata": {
    "tags": []
   },
   "source": [
    "SageMaker JumpStart provides pretrained, open-source models for a wide range of problem types to help you get started with machine learning. You can incrementally train and tune these models before deployment. JumpStart also provides solution templates that set up infrastructure for common use cases, and executable example notebooks for machine learning with SageMaker. You can also access JumpStart models using the SageMaker Python SDK. For information about how to use JumpStart models programmatically, see [Use SageMaker JumpStart Algorithms with Pretrained Models ](https://sagemaker.readthedocs.io/en/stable/overview.html#use-built-in-algorithms-with-pre-trained-models-in-sagemaker-python-sdk)\n",
    "\n",
    "You can access the pretrained models, solution templates, and examples through the JumpStart landing page in Amazon SageMaker Studio. \n",
    "\n",
    "This module presents a remarkable opportunity to explore the capabilities of Llama 2 model L in resolving language tasks such as abstractive question answering, text summarization, etc.\n",
    "\n",
    "To get started with the example, on the left-hand-side navigation pane, got to Home, under SageMaker JumpStart, choose Model, notebooks, solutions. Youâ€™re presented with a range of solutions, foundation models, and other artifacts that can help you get started with a specific model or a specific business problem or use case. If you want to experiment in a particular area, you can use the search function. Or you can simply browse the artifacts to find the relevant model or business solution for your needs. To start exploring the Llama 2 models, complete the following steps:\n",
    "\n",
    "TO deploy Llma 2- Go to the Foundation Models section. In the search bar, search for the llama model and select the Llama-2-7b. You can use the following screenshop to follow step by step. \n",
    "\n",
    "![image](./image.JPG)\n",
    "\n",
    "Click on view model and a new window will open where you can configure and delpy the model.\n",
    "\n",
    "Under Delpoyement configuration choose the instance type ml.g5.2xlarge, specify your endpoint name ( or leave as defualt) and then click Deploy.\n",
    "\n",
    "You can also deploy the model using the SageMaker Python SDK by clicking on the <mark>Notebook</mark> tab and opening the notebook that is shown.\n",
    "\n",
    "Once the model endpoint is in service, you can use the additional sample notebook to make inference using the deployed model.\n",
    "\n",
    "In this section we explored how we can deploy LLMs using SageMaker jumpstart that utilised the SageMaker Jumpstart containers. In the next section we will explore utilisation of HF Deep Learning Contianer to deploy suppoerted LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c2bee9-ea04-4c54-a959-fc82bf7e976b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section 2: Deploy Flan_T5_XXl using Hugging face DLC container.\n",
    "In this section, we will deploy the open-source Flan_T5_XXl model on SageMaker for real-time inference. For this deployment we willbe using Hugging Face LLM inference Deep Learning Containers (DLC).\n",
    "\n",
    "#### What is Hugging Face LLM Inference DLC?\n",
    "Hugging Face LLM DLC is a new purpose-built Inference Container to easily deploy LLMs in a secure and managed environment. The DLC is powered by Text Generation Inference (TGI), an open-source, purpose-built solution for deploying and serving Large Language Models (LLMs). TGI enables high-performance text generation using Tensor Parallelism and dynamic batching for the most popular open-source LLMs, including StarCoder, BLOOM, GPT-NeoX, Llama, and T5. Text Generation Inference is already used by customers such as IBM, Grammarly, and the Open-Assistant initiative implements optimization for all supported model architectures, including:\n",
    "\n",
    "* Tensor Parallelism and custom cuda kernels\n",
    "* Optimized transformers code for inference using [flash-attention](https://github.com/HazyResearch/flash-attention) on the most popular architectures\n",
    "* Quantization with [bitsandbytes](https://github.com/TimDettmers/bitsandbytes)\n",
    "* [Continuous batching of incoming requests](https://github.com/huggingface/text-generation-inference/tree/main/router) for increased total throughput\n",
    "* Accelerated weight loading (start-up time) with [safetensors](https://github.com/huggingface/safetensors)\n",
    "* Logits warpers (temperature scaling, topk, repetition penalty ...)\n",
    "* Watermarking with [A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226)\n",
    "* Stop sequences, Log probabilities\n",
    "* Token streaming using Server-Sent Events (SSE)\n",
    "\n",
    "Officially supported model architectures are currently: \n",
    "* [BLOOM](https://huggingface.co/bigscience/bloom) / [BLOOMZ](https://huggingface.co/bigscience/bloomz)\n",
    "* [MT0-XXL](https://huggingface.co/bigscience/mt0-xxl)\n",
    "* [Galactica](https://huggingface.co/facebook/galactica-120b)\n",
    "* [SantaCoder](https://huggingface.co/bigcode/santacoder)\n",
    "* [GPT-Neox 20B](https://huggingface.co/EleutherAI/gpt-neox-20b) (joi, pythia, lotus, rosey, chip, RedPajama, open assistant)\n",
    "* [FLAN-T5-XXL](https://huggingface.co/google/flan-t5-xxl) (T5-11B)\n",
    "* [Llama](https://github.com/facebookresearch/llama) (vicuna, alpaca, koala)\n",
    "* [Starcoder](https://huggingface.co/bigcode/starcoder) / [SantaCoder](https://huggingface.co/bigcode/santacoder)\n",
    "* [Falcon 7B](https://huggingface.co/tiiuae/falcon-7b) / [Falcon 40B](https://huggingface.co/tiiuae/falcon-40b)\n",
    "\n",
    "With the new Hugging Face LLM Inference DLCs on Amazon SageMaker, AWS customers can benefit from the same technologies that power highly concurrent, low latency LLM experiences like [HuggingChat](https://hf.co/chat), [OpenAssistant](https://open-assistant.io/), and Inference API for LLM models on the Hugging Face Hub. \n",
    "\n",
    "Lets get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade8a8fb-f154-4916-b0ed-289ebe41f3bc",
   "metadata": {},
   "source": [
    "To deploy FLAN_T5_XXL to Amazon SageMaker we set up our environemnt and define our endpoint configuration including the hf_model_id, instance_type etc. We will use a g5.12xlarge instance type. We then utilise the Hugging face DLC image by passing its relevant uri to create our model object, ready to deployed. \n",
    "This is an example on how to deploy the open-source LLMs, to Amazon SageMaker for inference using the new Hugging Face LLM Inference Container. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb004e0-544d-4714-a4e2-d03b629fe812",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "number_of_gpu = 4\n",
    "health_check_timeout = 900\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d28c37-91e2-4f46-bd51-94048f95e259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"google/flan-t5-xxl\", # model_id from hf.co/models\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(4096),  # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(5120),  # Max length of the generation (including input text)\n",
    "  #'HF_MODEL_QUANTIZE': \"bitsandbytes\", # comment in to quantize\n",
    "}\n",
    "\n",
    "\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"0.8.2\"\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  env=config\n",
    ")\n",
    "\n",
    "# Deploy model to an endpoint\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  # volume_size=400, # If using an instance with local SSD storage, volume_size must be None, e.g. p4 but not p3\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2fe094-593f-4ff9-adad-c059f64facc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFacePredictor\n",
    "\n",
    "# endpoint_name = \"flan-ul2-2047-2023-06-27-01-28-36-094\"\n",
    "endpoint_name = 'huggingface-pytorch-tgi-inference-2023-07-24-23-59-42-211'\n",
    "predictor = HuggingFacePredictor(endpoint_name=endpoint_name)\n",
    "\n",
    "parameters = {\n",
    "    \"max_length\": 500, # This is not used\n",
    "    \"max_new_tokens\": 500, # Default value\n",
    "    \"temperature\": 0.01,\n",
    "    \"top_p\": 0.1,\n",
    "}\n",
    "\n",
    "## promot template function\n",
    "def intent_template(text, predictor):\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        Extract the topic of the customer conversation \n",
    "        \"Input:\\n\\n{text}\"\n",
    "        Output:\n",
    "    \"\"\"\n",
    "    payload = prompt\n",
    "    print(prompt)\n",
    "\n",
    "    response = predictor.predict({\n",
    "    \"inputs\": payload,\n",
    "    \"parameters\" :parameters})\n",
    "\n",
    "    json_extraction = response[0][\"generated_text\"]\n",
    "\n",
    "    return json_extraction\n",
    "\n",
    "\n",
    "#this is how you would make infernece on your endpoint\n",
    "\n",
    "text= \"i have faced some issues when booking a flight from Melbourne to Amsterdam. When i was trying to use the online booking system, it times out and i coudln't get back to it?can you please help me?\" \n",
    "predictor=predictor\n",
    "parameters= {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.7,\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_k\": 50,\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"repetition_penalty\": 1.03,\n",
    "    \"stop\": [\"<|endoftext|>\"]\n",
    "  }\n",
    "\n",
    "print(intent_template(text, predictor))\n",
    "print(summary_template(text, predictor))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fe8f73-97f9-4a0a-945b-eef17152a943",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The following shows you how you would envoke the Flan model directly from the endpoint without using the restful API request that you can explore on your own time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48f0150-f33d-4ea7-9610-20f5c1d89a23",
   "metadata": {},
   "source": [
    "## Section 3- Prompting the FLAN_T5_XXL to extract the intents.\n",
    "In the following we will be interacting with the deloyed Flan_T5_XXl and use it to extract Chatbot conversations intents. \n",
    "We first define a few simple prompt template using text strings that will allow for user input text (which is the chat bot conversation in this scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bc8496-7143-40ea-a7d2-749e11c7246d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def query_endpoint_with_json_payload(url, data, payload):\n",
    "    response = requests.post(\n",
    "        url,\n",
    "        headers=data,\n",
    "        json=payload,\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# def parse_response_multiple_texts(query_response):\n",
    "#     return query_response.json()[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52107d0-f175-4a3a-bd68-99cf9c0ed160",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"Which instances can I use with Managed Spot Training in SageMaker?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58765ee7-518c-46d7-8be5-e893c1148003",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_MODEL_CONFIG_ = {\n",
    "        \"Flan_T5_XXL\" : {\n",
    "        \"aws_region\": \"us-east-1\",\n",
    "        \"endpoint_name\": \"demo-FlanT5-Endpoint\",\n",
    "        \"api_url\": \"https://kj72lukej0.execute-api.us-east-1.amazonaws.com/prod/flan\",\n",
    "        \"headers\":{\n",
    "    'Content-Type': 'application/json',\n",
    "    'Accept': 'application/json',\n",
    "    'Authorization':'xxx'  #insert the authentication code\n",
    "}, \n",
    "        # \"parse_function\": parse_response_multiple_texts,\n",
    "        # \"prompt\": \"\"\"{context}\\n\\nGiven the above context, answer the following question:\\n{question}\\nAnswer: \"\"\",\n",
    "\n",
    "    },\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    \"text_inputs\": question,\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.7,\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_k\": 50,\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"repetition_penalty\": 1.03\n",
    "}\n",
    "#print(payload)\n",
    "model_id=\"Flan_T5_XXL\"\n",
    "api_url = _MODEL_CONFIG_[model_id][\"api_url\"]\n",
    "data=  _MODEL_CONFIG_[model_id][\"headers\"]\n",
    "\n",
    "query_response = query_endpoint_with_json_payload(\n",
    "        api_url,\n",
    "        data,\n",
    "        payload,\n",
    "        )\n",
    "import json\n",
    "\n",
    "print(query_response.__attrs__)\n",
    "print(json.dumps(query_response.json(), indent=2))\n",
    "\n",
    "\n",
    "# generated_texts = _MODEL_CONFIG_[model_id][\"parse_function\"](query_response)\n",
    "# print(f\"For model: {model_id}, the generated output is: {generated_texts}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dff1704-c897-4fb0-806c-7ddf2d04cfe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text= \"I have faced some issues when booking a flight from Melbourne to Amsterdam. When i was trying to use the online booking system, it times out and i coudln't get back to it?can you please help me?\" \n",
    "\n",
    "parameters= {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.7,\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_k\": 50,\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"repetition_penalty\": 1.03,\n",
    "     }\n",
    "text_inputs = f\"\"\"\n",
    "    Extract the topic of the customer conversation \n",
    "    \"Input:\\n\\n{text}\"\n",
    "    Output:\n",
    "    \"\"\"\n",
    "payload={}\n",
    "payload['text_inputs']= text_inputs\n",
    "\n",
    "\n",
    "parameters= parameters \n",
    "\n",
    "payload.update(parameters)\n",
    "#print(payload)\n",
    "\n",
    "query_response = query_endpoint_with_json_payload(\n",
    "        api_url,\n",
    "        data,\n",
    "        payload,\n",
    "        )\n",
    "print(json.dumps(query_response.json(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbad29cb-e6d5-499d-b00c-7f3ff62b741d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def intent_template(text, parameters, query_response):\n",
    "    \n",
    "    parameters= parameters\n",
    "    text_inputs = f\"\"\"\n",
    "    Extract the topic of the customer conversation \n",
    "    \"Input:\\n\\n{text}\"\n",
    "    Output:\n",
    "    \"\"\",\n",
    "    payload={}\n",
    "    payload['text_inputs']= text_inputs\n",
    "     \n",
    "    payload.update(parameters)\n",
    "\n",
    "    query_response = query_endpoint_with_json_payload(\n",
    "        api_url,\n",
    "        data,\n",
    "        payload,\n",
    "        )\n",
    "\n",
    "    json_extraction = json.dumps(query_response.json(), indent=2)\n",
    "\n",
    "    return json_extraction\n",
    "\n",
    "\n",
    "\n",
    "def summary_template(text, parameters, query_response):\n",
    "    \n",
    "    text_inputs = f\"\"\"\n",
    "    Provide a short summary of what is it that the customer contacted for?do not provide answers\n",
    "    \"Input:\\n\\n{text}\"\n",
    "    Output:\n",
    "    \"\"\",\n",
    "    payload={}\n",
    "    payload['text_inputs']= text_inputs\n",
    "    parameters= parameters \n",
    "    payload.update(parameters)\n",
    "\n",
    "    query_response = query_endpoint_with_json_payload(\n",
    "        api_url,\n",
    "        data,\n",
    "        payload,\n",
    "        )\n",
    "\n",
    "    json_extraction = json.dumps(query_response.json(), indent=2)\n",
    "\n",
    "    return json_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52cef31-5438-4a5d-85e1-b0085a360c2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text= \"I have faced some issues when booking a flight from Melbourne to Amsterdam. When i was trying to use the online booking system, it times out and i coudln't get back to it?can you please help me?\" \n",
    "\n",
    "parameters= {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.7,\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_k\": 50,\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"repetition_penalty\": 1.03,\n",
    "     }\n",
    "\n",
    "print(intent_template(text, parameters, query_response))\n",
    "print(summary_template(text, parameters, query_response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1746e8c0-01f2-43a9-967d-20a53592d3f8",
   "metadata": {},
   "source": [
    "## Section 4- Prompt Engineering with spcific intent lables.\n",
    "In this section we will perform additional prompt engineering, to pass on the specific intent lables to the model and ask the model to pick one of those intents when extracting the intent from the chat session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40c88ab-ca51-41bd-965b-85cbf137012b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c942e2b-91d3-4eac-a9e5-53f9d5776c67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str=\"cl100k_base\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c8b023-6021-4d0a-888c-637cdcb53bfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reasons_df = pd.read_csv(\"genai-workshop/data/Reasons.csv\")\n",
    "reasons_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44b3c08-37a8-4340-b122-e5d1c29f2096",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reasons_df['intent'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28226c8-9031-4244-b88e-59b2b6ad8de7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reason_tree = reasons_df.groupby('intent')['intent'].unique().apply(list).to_dict()\n",
    "sub_intent_tree = reasons_df.groupby('intent')['sub_intent'].unique().apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ca2abd-c83f-4fca-9c44-9f48aef3d667",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(list(reason_tree.values())[:5])\n",
    "print(list(sub_intent_tree.values())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fc58fd-efe6-4e56-a3f9-7ffeaf4e7895",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def intent_template(candidate_labels, customer_feedback, parameters, query_response):\n",
    "    \n",
    "    parameters= parameters\n",
    "    \n",
    "    text_inputs = prompt = f\"\"\"\n",
    "        Classify the input text only from the labels listed below\n",
    "        \"Labels\": {candidate_labels}\n",
    "        \"Input\": {customer_feedback}\n",
    "        \"Output\":\"\"\"\n",
    "    \n",
    "    payload={}\n",
    "    \n",
    "    payload['text_inputs']= text_inputs\n",
    "     \n",
    "    payload.update(parameters)\n",
    "\n",
    "    query_response = query_endpoint_with_json_payload(\n",
    "        api_url,\n",
    "        data,\n",
    "        payload,\n",
    "        )\n",
    "\n",
    "    json_extraction = json.dumps(query_response.json(), indent=2)\n",
    "\n",
    "    return json_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3bf20a-160a-4328-ae2c-32499f1d56a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "conv= \"I have just landed and can not find my luggage. \"\n",
    "\n",
    "parameters= {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.7,\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_k\": 50,\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"repetition_penalty\": 1.03,\n",
    "     }\n",
    "\n",
    "call_intent = intent_template(list(reason_tree.keys()), conv, parameters, query_response)\n",
    "call_sub_intent = intent_template(sub_intent_tree.get(json.loads(call_intent).get(\"generated_texts\")[0]), conv, parameters, query_response)\n",
    "\n",
    "results.append({\n",
    "        \"call_intent\":call_intent, \n",
    "         \"call_sub_intent\":call_sub_intent\n",
    "           })\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c22e94b-97b1-4f98-a2b3-ec354fa9325d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
